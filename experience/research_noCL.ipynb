{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Experience"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from utils import BasicDataset\n",
    "from unet import UNet\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from avalanche.benchmarks.scenarios.dataset_scenario import benchmark_from_datasets\n",
    "from avalanche.evaluation.metrics import WeightCheckpoint, gpu_usage_metrics, \\\n",
    "    loss_metrics, timing_metrics, cpu_usage_metrics, disk_usage_metrics\n",
    "from avalanche.benchmarks.utils import AvalancheDataset\n",
    "from avalanche.logging import InteractiveLogger, TextLogger, WandBLogger, TensorboardLogger\n",
    "from avalanche.training.plugins import EvaluationPlugin\n",
    "from avalanche.training.supervised import Naive,EWC\n",
    "from avalanche.benchmarks.scenarios.supervised import class_incremental_benchmark\n",
    "from torch import optim\n",
    "import numpy as np\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from avalanche.benchmarks import benchmark_from_datasets\n",
    "from avalanche.benchmarks.utils.data_attribute import DataAttribute\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "aval_datasets = []\n",
    "for i in range(5):\n",
    "    train_dataset = BasicDataset(images_dir=f\"./stack_data/train/img/{i}\",\n",
    "                                mask_dir=f\"./stack_data/train/label/{i}\")\n",
    "    aval_dataset = AvalancheDataset([train_dataset],\n",
    "                                    data_attributes=[\n",
    "                                                    DataAttribute([0] * len(train_dataset), \"targets_task_labels\"),\n",
    "                                                    ])\n",
    "    aval_datasets.append(aval_dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bm = benchmark_from_datasets(train=aval_datasets,\n",
    "                             test=[aval_datasets[0]]*len(aval_datasets))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"original training samples = {len(bm.train_stream[0].dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = 1\n",
    "batch_size= 8\n",
    "learning_rate = 0.00001\n",
    "weight_decay = 0.00000001\n",
    "momentum = 0.999"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train using NAIVE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "func_name = \"NAIVE_all\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = UNet(n_channels=3, n_classes=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# DEFINE THE EVALUATION PLUGIN and LOGGERS\n",
    "# The evaluation plugin manages the metrics computation.\n",
    "# It takes as argument a list of metrics, collectes their results and returns\n",
    "# them to the strategy it is attached to.\n",
    "\n",
    "# log to Tensorboard\n",
    "tb_logger = TensorboardLogger(f\"./log/tb_data/{func_name}\")\n",
    "\n",
    "wandb_logger = WandBLogger(project_name=\"UNet_CL\",\n",
    "                           run_name=func_name,\n",
    "                           path=f\"./log/checkpoint/{func_name}\",\n",
    "                           dir=f\"./log/wandb/{func_name}\")\n",
    "\n",
    "# log to text file\n",
    "text_logger = TextLogger(open(f'./log/{func_name}.txt', 'a'))\n",
    "\n",
    "# print to stdout\n",
    "interactive_logger = InteractiveLogger()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# CREATE THE STRATEGY INSTANCE (NAIVE)\n",
    "optimizer = optim.RMSprop(model.parameters(),\n",
    "                          lr=learning_rate, \n",
    "                          weight_decay=weight_decay, \n",
    "                          momentum=momentum, \n",
    "                          foreach=True)\n",
    "\n",
    "eval_plugin = EvaluationPlugin(\n",
    "    loss_metrics(minibatch=True, epoch=True, experience=True, stream=True),\n",
    "    timing_metrics(epoch=True, epoch_running=True),\n",
    "    cpu_usage_metrics(experience=True),\n",
    "    disk_usage_metrics(minibatch=True, epoch=True, experience=True, stream=True),\n",
    "    gpu_usage_metrics(gpu_id=0, minibatch=True, epoch=True, experience=True, stream=True),\n",
    "    loggers=[interactive_logger, text_logger, tb_logger, wandb_logger]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from avalanche.training import Naive\n",
    "cl_strategy = Naive(\n",
    "    model=model, \n",
    "    optimizer=optimizer,\n",
    "    criterion=CrossEntropyLoss(), \n",
    "    train_mb_size=batch_size, \n",
    "    train_epochs=epochs,\n",
    "    device=\"cuda:0\",\n",
    "    eval_mb_size=batch_size,\n",
    "    evaluator=eval_plugin)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.init as init\n",
    "\n",
    "# 在某个时刻，你想重新初始化模型的参数\n",
    "def reinitialize_model(model, initialization_method=\"normal\", mean=0, std=0.02):\n",
    "    for name, param in model.named_parameters():\n",
    "        if \"weight\" in name:\n",
    "            if initialization_method == \"normal\":\n",
    "                init.normal_(param, mean=mean, std=std)\n",
    "            elif initialization_method == \"uniform\":\n",
    "                init.uniform_(param, a=-std, b=std)  # 注意调整a和b以匹配你的需求\n",
    "            elif initialization_method == \"xavier\":\n",
    "                init.xavier_normal_(param)\n",
    "            elif initialization_method == \"kaiming\":\n",
    "                init.kaiming_normal_(param, nonlinearity='relu')\n",
    "            else:\n",
    "                raise ValueError(f\"Unsupported initialization method: {initialization_method}\")\n",
    "        elif \"bias\" in name:\n",
    "            init.zeros_(param)  # 通常将偏置初始化为0\n",
    "\n",
    "# 使用指定的方法重新初始化模型参数\n",
    "reinitialize_model(model, initialization_method=\"normal\")  # 使用正态分布重新初始化权重\n",
    "\n",
    "# 如果需要，可以多次调用此函数以应用不同的初始化策略"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAINING LOOP\n",
    "print('Starting experiment...')\n",
    "results = []\n",
    "for i, experience in enumerate(bm.train_stream):\n",
    "# if True:\n",
    "    # print(\"Start of experience: \", experience.current_experience)\n",
    "    # print(\"Current Classes: \", experience.classes_in_this_experience)\n",
    "\n",
    "    # train returns a dictionary which contains all the metric values\n",
    "    res = cl_strategy.train(experience)\n",
    "    print('Training completed')\n",
    "\n",
    "    print('Computing accuracy on the whole test set')\n",
    "    # test also returns a dictionary which contains all the metric values\n",
    "    results.append(cl_strategy.eval(bm.test_stream[i]))\n",
    "    reinitialize_model(cl_strategy.model, initialization_method=\"normal\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "pickle.dump(results,open(f\"./log/eval/{func_name}.pkl\",\"wb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.finish()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
